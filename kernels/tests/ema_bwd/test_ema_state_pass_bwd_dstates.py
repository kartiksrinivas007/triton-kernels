import torch
import triton
import triton.language as tl
import numpy as np
from einops import rearrange, repeat

# from kernels.ema_kernels_bwd.ema_sc
from kernels.ema_kernels.ema_cumsum import ema_chunk_cumsum_fwd
from kernels.ema_kernels_bwd.ema_scan_bwd import _ema_chunk_scan_bwd_dstates
from kernels.ema_kernels_bwd.ema_state_passing_bwd_dstates import _ema_state_passing_bwd

from kernels.mamba_kernels_bwd.mamba_scan_bwd import _chunk_scan_bwd_dstates
from kernels.mamba_kernels.mamba_cumsum import _chunk_cumsum_fwd
from kernels.mamba_kernels_bwd.mamba_state_passing_bwd_dstates import _state_passing_bwd

import triton.runtime.driver as driver

def _get_gpu_specifications(DEVICE):

    assert torch.cuda.is_available(), "CUDA must be avialble to run triton kernels"

    properties = driver.active.utils.get_device_properties(DEVICE.index)  # type:ignore
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]

    def is_cuda():
        return (
            triton.runtime.driver.active.get_current_target().backend  # type: ignore
            == "cuda"
        )

    def supports_host_descriptor():
        return is_cuda() and torch.cuda.get_device_capability()[0] >= 9

    print("=" * 100)
    print("Device = ", DEVICE)
    print("Num REGS per SM  = ", NUM_REGS)
    print("Num SM  = ", NUM_SM)
    print("Total Shared memory (bytes) = ", SIZE_SMEM)
    print("Warp size = ", WARP_SIZE)
    print("Supports Host Descriptor = ", supports_host_descriptor())
    print("=" * 100)

    return DEVICE, properties

class TestEmaCumsumKernels:
    BATCH_SIZE = 8
    SEQLEN = 8192
    HEAD_DIM = 512
    MAMBA_HEAD_DIM = 32
    N_HEADS = 16
    DTYPE = torch.float32
    MAMBA_CHUNK_SIZE = 64 # the chunking level? 
    NUM_CHUNKS = (SEQLEN + MAMBA_CHUNK_SIZE - 1) // MAMBA_CHUNK_SIZE


    @classmethod
    def setup_class(cls):

        DEVICE = driver.active.get_active_torch_device()  # type: ignore
        _, properties = _get_gpu_specifications(DEVICE)
        target = triton.runtime.driver.active.get_current_target()
        X = torch.randn((cls.BATCH_SIZE, cls.SEQLEN, cls.HEAD_DIM), dtype=torch.float32, device=DEVICE)
        P = torch.rand((cls.BATCH_SIZE, cls.SEQLEN, 1), dtype=torch.float32, device=DEVICE)
        Z = torch.empty_like(X)  # same shape as X, but smoothed according to P
        dout = torch.randn_like(Z) # should have the same shape as Z
        states = torch.randn_like(Z)


        # input variables for the mamba kernel
        cls.dt = -torch.log(1 - P).to(torch.float32).squeeze(-1)
        cls.X_beta = X / cls.dt[..., None]
        cls.X_m = rearrange(cls.X_beta, "b l (h p) -> b l h p", p=cls.MAMBA_HEAD_DIM)
        cls.dt = repeat(cls.dt, "b l -> b l h", h=cls.N_HEADS)
        cls.A = -1 * torch.ones(cls.N_HEADS, dtype=torch.float32, device=DEVICE)
        cls.B_m = rearrange(P.to(torch.float32), "b l 1 -> b l 1 1")
        cls.C_m = torch.ones_like(cls.B_m)
        cls.dout_m = rearrange(dout, "b l (h p) -> b l h p", p = cls.MAMBA_HEAD_DIM)
        cls.states_m = rearrange(states, "b l (h p) -> b l h p", p = cls.MAMBA_HEAD_DIM)

        cls.A_ema = torch.log(1 - P).squeeze(-1) # the final dimension
        cls.ema_dout= dout
        cls.ema_states = states

        # modified input variables for the ema kernel
        # Steps 
        # 1. Define the matrices needed for the kernel A and dt
        # 1.2 Call the main kernel and see if the main kernel call works
        # 2. construct the inputs for both the kernels
        # 3. Write ema kernel for ema input
        # 3.5 Test if the ema kernel works in the basic case
        # 4. Write code to benchmark both kernels
        
        
    def test_ema_state_passing_bwd_matches_mamba(self):
        """
        Check that EMA state-passing backward produces the same gradients
        as the Mamba implementation when we flatten (head, dim) into a
        single token_dim and use a head-independent dA_chunk_cumsum.
        """
        DEVICE = driver.active.get_active_torch_device()  # type: ignore

        batch = self.BATCH_SIZE
        nchunks = self.NUM_CHUNKS
        nheads = self.N_HEADS
        headdim = self.MAMBA_HEAD_DIM
        token_dim = self.HEAD_DIM  # nheads * headdim

        # random states and upstream gradients at chunk level
        states_mamba = torch.randn(
            batch, nchunks, nheads, headdim, device=DEVICE, dtype=self.DTYPE
        )
        dout_mamba = torch.randn_like(states_mamba)

        # use the same dA_chunk_cumsum across heads so flattening is valid
        dA_base = torch.randn(batch, nchunks, device=DEVICE, dtype=self.DTYPE)
        dA_mamba = dA_base[:, None, :].expand(batch, nheads, nchunks)

        # Mamba backward over chunks
        new_mamba_dstates, ddA_mamba, dinit_mamba, states_conv_mamba = _state_passing_bwd(  # type: ignore
            states_mamba,
            dA_mamba,
            dout_mamba,
            dfinal_states=None,
            seq_idx=None,
            has_initial_states=False,
            dstates_dtype=dout_mamba.dtype,
            states_dtype=states_mamba.dtype,
            chunk_size=self.MAMBA_CHUNK_SIZE,
        )

        # Flatten (head, dim) into token_dim for EMA
        states_ema = rearrange(states_mamba, "b c h p -> b c (h p)")
        dout_ema = rearrange(dout_mamba, "b c h p -> b c (h p)")
        assert states_ema.shape == (batch, nchunks, token_dim)
        assert dout_ema.shape == (batch, nchunks, token_dim)

        # EMA backward over chunks
        new_ema_dstates, ddA_ema, dinit_ema, states_conv_ema = _ema_state_passing_bwd(  # type: ignore
            states_ema,
            dA_base,
            dout_ema,
            dfinal_states=None,
            seq_idx=None,
            has_initial_states=False,
            dstates_dtype=dout_ema.dtype,
            states_dtype=states_ema.dtype,
            chunk_size=self.MAMBA_CHUNK_SIZE,
        )

        # Compare dstates: flatten Mamba's (head, dim) and match EMA
        new_mamba_dstates_flat = rearrange(new_mamba_dstates, "b c h p -> b c (h p)")
        assert torch.allclose(new_ema_dstates, new_mamba_dstates_flat, atol=1e-2, rtol=1e-2)

        # Compare dA gradients: EMA aggregates over all heads, so sum Mamba's ddA over heads
        ddA_mamba_agg = ddA_mamba.sum(dim=1)  # shape: (batch, nchunks)
        assert torch.allclose(ddA_ema, ddA_mamba_agg, atol=1e-2, rtol=1e-2)
